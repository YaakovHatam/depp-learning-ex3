# -*- coding: utf-8 -*-
"""dl2023_class.ex3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19vftf7MaHVxr4bJZog1MrgoyoDNqcGcY
"""

import numpy as np
from keras.datasets import cifar10

# Load CIFAR10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Preprocess data
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

import numpy as np
import matplotlib.pyplot as plt

# Flatten the data
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# Define the network parameters
n_input = x_train.shape[1]
n_hidden = 64
n_output = 10

# Initialize the weights and biases
W1 = np.random.randn(n_input, n_hidden)
b1 = np.zeros(n_hidden)
W2 = np.random.randn(n_hidden, n_output)
b2 = np.zeros(n_output)

# Define the ReLU activation function
def relu(x):
    return np.maximum(0, x)

# Define the derivative of the ReLU function
def relu_derivative(x):
    return np.where(x > 0, 1, 0)

# Define the softmax function
def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)

# Define the cross entropy loss function
def cross_entropy(y_pred, y_true):
    return -np.sum(y_true * np.log(y_pred))

# Training loop
epochs = 1000
learning_rate = 0.001
lambda_ = 0.01
losses = []
batch_size = 100  # Define the batch size

for i in range(epochs):
    # Shuffle the data
    permutation = np.random.permutation(x_train.shape[0])
    x_train_shuffled = x_train[permutation]
    y_train_shuffled = y_train[permutation]

    for j in range(0, x_train.shape[0], batch_size):
        # Get the mini-batch
        x_batch = x_train_shuffled[j:j+batch_size]
        y_batch = y_train_shuffled[j:j+batch_size]

        # Forward propagation
        z1 = np.dot(x_batch, W1) + b1
        a1 = relu(z1)
        z2 = np.dot(a1, W2) + b2
        y_pred = softmax(z2)

        # Compute the loss
        loss = cross_entropy(y_pred, y_batch)
        losses.append(loss)

        # Backward propagation
        delta_y = y_pred - y_batch
        delta_hidden = np.dot(delta_y, W2.T) * relu_derivative(a1)

        # Update the weights and biases
        W2 -= learning_rate * np.dot(a1.T, delta_y) + lambda_ * W2
        b2 -= learning_rate * np.sum(delta_y, axis=0)
        W1 -= learning_rate * np.dot(x_batch.T, delta_hidden) + lambda_ * W1
        b1 -= learning_rate * np.sum(delta_hidden, axis=0)

        # Print the loss every 100 epochs
        if j % 1 == 0:
            print(f'Epoch {j}, Loss: {loss}')

# Plot the loss
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

